#!/usr/bin/env python3
"""
MultiVAE Regularization ì™„í™” ì‹¤í—˜

ë¬¸ì œ: í˜„ì¬ MultiVAEëŠ” ë‹¨ 10ê°œ ì•„ì´í…œë§Œ ì¶”ì²œ (ì‹¬ê°í•œ overfitting)
ì›ì¸: ê³¼ë„í•œ regularization
- Dropout: 0.519 (ë„ˆë¬´ ë†’ìŒ)
- Learning rate: 5.34e-05 (ë„ˆë¬´ ë‚®ìŒ)
- Anneal cap: 0.4 (ë„ˆë¬´ ë†’ìŒ)

í•´ê²°: Regularization ì™„í™” + ì „ì²´ ë°ì´í„° í™œìš©
- Dropout: 0.2~0.4
- Learning rate: 1e-4~5e-3
- Anneal cap: 0.1~0.2
- Data split: 90/10/0 (ì „ì²´ ë°ì´í„° ìµœëŒ€ í™œìš©)

ëª©í‘œ: ì¶”ì²œ ë‹¤ì–‘ì„± 10ê°œ â†’ 100ê°œ+, Public LB 0.197 â†’ 0.20+
"""

import os
import pandas as pd
import numpy as np
import torch
from pathlib import Path
from datetime import datetime

from recbole.config import Config
from recbole.data import create_dataset, data_preparation
from recbole.utils import init_seed

from ray import tune, train, air
from ray.air import session
from ray.tune.search.optuna import OptunaSearch
from ray.tune.schedulers import ASHAScheduler

print("=" * 80)
print("MultiVAE Regularization ì™„í™” ì‹¤í—˜")
print("=" * 80)
print()

# ============================================================
# 1. ë°ì´í„° ì¤€ë¹„
# ============================================================
print("=" * 80)
print("1. ë°ì´í„° ì¤€ë¹„")
print("=" * 80)
print()

train_file = 'dataset/comp_train.csv'
df = pd.read_csv(train_file)
df.columns = [col.replace('\ufeff', '') for col in df.columns]

print(f"ì›ë³¸ ë°ì´í„°: {len(df):,}ê°œ interactions")
print(f"ì‚¬ìš©ì: {df['user_id'].nunique():,}ëª…")
print(f"ì•„ì´í…œ: {df['item_id'].nunique():,}ê°œ")
print()

# RecBole í˜•ì‹ ë³€í™˜
df_recbole = pd.DataFrame({
    'user_id:token': df['user_id'],
    'item_id:token': df['item_id'],
    'rating:float': 1.0
})

dataset_dir = 'dataset/kaggle_recsys'
os.makedirs(dataset_dir, exist_ok=True)
inter_file = os.path.join(dataset_dir, 'kaggle_recsys.inter')
df_recbole.to_csv(inter_file, sep='\t', index=False)

print(f"âœ… RecBole ë°ì´í„°ì…‹ ìƒì„±: {inter_file}")
print()

# ============================================================
# 2. Ray Tune Training Function
# ============================================================

DATASET_PATH = str(Path(__file__).parent / 'dataset')

def train_multivae_with_config(config_hyperparams):
    """Ray Tune training function"""

    # Base config (90/10/0 split - ì „ì²´ ë°ì´í„° ìµœëŒ€ í™œìš©)
    base_config = {
        'data_path': DATASET_PATH,
        'dataset': 'kaggle_recsys',
        'USER_ID_FIELD': 'user_id',
        'ITEM_ID_FIELD': 'item_id',
        'RATING_FIELD': 'rating',
        'load_col': {'inter': ['user_id', 'item_id', 'rating']},
        'eval_args': {
            'split': {'RS': [0.9, 0.1, 0.0]},  # 90% train, 10% validation, 0% test
            'order': 'RO',
            'mode': 'full',
            'group_by': 'user'
        },
        'metrics': ['Recall', 'NDCG', 'MRR'],
        'topk': [5, 10, 20],
        'valid_metric': 'Recall@5',
        'epochs': 100,
        'stopping_step': 10,
        'train_batch_size': 4096,
        'eval_batch_size': 102400,
        'seed': 2024,
        'reproducibility': True,
    }

    # Merge with hyperparameters
    full_config = {**base_config, **config_hyperparams}

    # RecBole config
    config = Config(model='MultiVAE', config_dict=full_config)
    init_seed(config['seed'], config['reproducibility'])

    # Dataset
    dataset = create_dataset(config)
    train_data, valid_data, test_data = data_preparation(config, dataset)

    # Model
    from recbole.model.general_recommender import MultiVAE
    model = MultiVAE(config, train_data.dataset).to(config['device'])

    # Trainer
    from recbole.trainer import Trainer
    trainer = Trainer(config, model)

    # Train
    best_valid_score, best_valid_result = trainer.fit(
        train_data, valid_data, saved=False, show_progress=False
    )

    # Report to Ray Tune
    train.report({
        'recall@5': best_valid_result['recall@5'],
        'ndcg@5': best_valid_result['ndcg@5'],
        'mrr@5': best_valid_result['mrr@5'],
    })

# ============================================================
# 3. Ray Tune ì„¤ì •
# ============================================================
print("=" * 80)
print("2. Ray Tune Hyperparameter Search ì„¤ì •")
print("=" * 80)
print()

# ê°œì„ ëœ Search Space (Regularization ì™„í™”)
search_space = {
    # í•µì‹¬ ë³€ê²½: Regularization ì™„í™”
    'dropout_prob': tune.uniform(0.2, 0.4),         # 0.25~0.65 â†’ 0.2~0.4
    'learning_rate': tune.loguniform(1e-4, 5e-3),   # 5e-5~5e-3 â†’ 1e-4~5e-3
    'anneal_cap': tune.choice([0.1, 0.2]),          # 0.1~0.4 â†’ 0.1~0.2

    # ê¸°ì¡´ ìœ ì§€
    'latent_dimension': tune.choice([128, 200, 256]),
    'mlp_hidden_size': tune.choice([[600], [512]]),
}

print("ğŸ“Š Search Space (Regularization ì™„í™”):")
print(f"   dropout_prob: 0.2~0.4 (ì´ì „: 0.25~0.65)")
print(f"   learning_rate: 1e-4~5e-3 (ì´ì „: 5e-5~5e-3)")
print(f"   anneal_cap: [0.1, 0.2] (ì´ì „: [0.1, 0.2, 0.3, 0.4])")
print(f"   latent_dimension: [128, 200, 256]")
print(f"   mlp_hidden_size: [[600], [512]]")
print()

print("ğŸ¯ ëª©í‘œ:")
print(f"   ì¶”ì²œ ë‹¤ì–‘ì„±: 10ê°œ â†’ 100ê°œ+")
print(f"   Validation Recall@5: 0.0865 â†’ 0.10+")
print(f"   Public LB: 0.197 â†’ 0.20+")
print()

# Optuna TPE Sampler
optuna_search = OptunaSearch(
    metric='recall@5',
    mode='max'
)

# ASHA Scheduler (metric/mode will be set in TuneConfig)
asha_scheduler = ASHAScheduler(
    max_t=100,
    grace_period=10,
    reduction_factor=2
)

# ============================================================
# 4. Ray Tune ì‹¤í–‰
# ============================================================
print("=" * 80)
print("3. Ray Tune ì‹¤í–‰ ì‹œì‘")
print("=" * 80)
print()

# Ray init with /tmp/ray directory
import ray
ray.init(_temp_dir='/tmp/ray', ignore_reinit_error=True)

num_samples = 30
print(f"Trials: {num_samples}")
print(f"ì˜ˆìƒ ì‹œê°„: 1-2ì‹œê°„")
print()

tuner = tune.Tuner(
    train_multivae_with_config,
    param_space=search_space,
    tune_config=tune.TuneConfig(
        search_alg=optuna_search,
        scheduler=asha_scheduler,
        num_samples=num_samples,
        metric='recall@5',
        mode='max',
    ),
    run_config=air.RunConfig(
        name='multivae_relaxed_regularization',
        stop={'training_iteration': 1},
        verbose=1,
    )
)

results = tuner.fit()

# ============================================================
# 5. Best Result ë¶„ì„
# ============================================================
print()
print("=" * 80)
print("4. Best Result ë¶„ì„")
print("=" * 80)
print()

best_result = results.get_best_result(metric='recall@5', mode='max')
best_config = best_result.config
best_metrics = best_result.metrics

print(f"ğŸ† Best Hyperparameters:")
print(f"   dropout_prob: {best_config['dropout_prob']:.4f}")
print(f"   learning_rate: {best_config['learning_rate']:.6f}")
print(f"   anneal_cap: {best_config['anneal_cap']}")
print(f"   latent_dimension: {best_config['latent_dimension']}")
print(f"   mlp_hidden_size: {best_config['mlp_hidden_size']}")
print()

print(f"ğŸ† Best Validation Metrics:")
print(f"   Recall@5: {best_metrics['recall@5']:.4f}")
print(f"   NDCG@5: {best_metrics['ndcg@5']:.4f}")
print(f"   MRR@5: {best_metrics['mrr@5']:.4f}")
print()

# ì´ì „ ê²°ê³¼ì™€ ë¹„êµ
print(f"ğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
print(f"   ì´ì „ (v3): Recall@5 = 0.0865, NDCG@5 = 0.0576")
print(f"   í˜„ì¬ (relaxed): Recall@5 = {best_metrics['recall@5']:.4f}, NDCG@5 = {best_metrics['ndcg@5']:.4f}")
improvement = (best_metrics['recall@5'] - 0.0865) / 0.0865 * 100
print(f"   ê°œì„ ë„: {improvement:+.1f}%")
print()

# ============================================================
# 6. Best Model ì¬í•™ìŠµ ë° ì œì¶œ íŒŒì¼ ìƒì„±
# ============================================================
print("=" * 80)
print("5. Best Model ì¬í•™ìŠµ ë° ì œì¶œ íŒŒì¼ ìƒì„±")
print("=" * 80)
print()

# Best configë¡œ ì¬í•™ìŠµ
final_config_dict = {
    'data_path': DATASET_PATH,
    'dataset': 'kaggle_recsys',
    'USER_ID_FIELD': 'user_id',
    'ITEM_ID_FIELD': 'item_id',
    'RATING_FIELD': 'rating',
    'load_col': {'inter': ['user_id', 'item_id', 'rating']},
    'eval_args': {
        'split': {'RS': [0.9, 0.1, 0.0]},
        'order': 'RO',
        'mode': 'full',
        'group_by': 'user'
    },
    'metrics': ['Recall', 'NDCG', 'MRR'],
    'topk': [5, 10, 20],
    'valid_metric': 'Recall@5',
    'epochs': 100,
    'stopping_step': 10,
    'train_batch_size': 4096,
    'eval_batch_size': 102400,
    'seed': 2024,
    'reproducibility': True,

    # Best hyperparameters
    'dropout_prob': best_config['dropout_prob'],
    'learning_rate': best_config['learning_rate'],
    'anneal_cap': best_config['anneal_cap'],
    'latent_dimension': best_config['latent_dimension'],
    'mlp_hidden_size': best_config['mlp_hidden_size'],
}

config = Config(model='MultiVAE', config_dict=final_config_dict)
init_seed(config['seed'], config['reproducibility'])

dataset = create_dataset(config)
train_data, valid_data, test_data = data_preparation(config, dataset)

print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
print(f"   Train: {len(train_data.dataset):,}ê°œ")
print(f"   Valid: {len(valid_data.dataset):,}ê°œ")
print()

# Model
from recbole.model.general_recommender import MultiVAE
model = MultiVAE(config, train_data.dataset).to(config['device'])

# Trainer
from recbole.trainer import Trainer
trainer = Trainer(config, model)

# Train
print("í•™ìŠµ ì‹œì‘...")
best_valid_score, best_valid_result = trainer.fit(train_data, valid_data, saved=True, show_progress=True)

print()
print(f"âœ… í•™ìŠµ ì™„ë£Œ")
print(f"   Best Validation Recall@5: {best_valid_result['recall@5']:.4f}")
print()

# ============================================================
# 7. ì œì¶œ íŒŒì¼ ìƒì„±
# ============================================================
print("=" * 80)
print("6. ì œì¶œ íŒŒì¼ ìƒì„±")
print("=" * 80)
print()

# ì „ì²´ ì‚¬ìš©ìì— ëŒ€í•œ ì¶”ì²œ ìƒì„±
all_users = dataset.inter_feat['user_id'].unique()
print(f"ì „ì²´ ì‚¬ìš©ì ìˆ˜: {len(all_users):,}ëª…")

recommendations = []

for user_internal in all_users:
    user_external = dataset.id2token('user_id', user_internal.item())

    # User tensor
    user_tensor = torch.tensor([user_internal.item()]).to(config['device'])

    # Predict scores
    with torch.no_grad():
        scores = model.full_sort_predict(user_tensor)

    # Top 5
    _, top_indices = torch.topk(scores, k=5)
    top_items_internal = top_indices.cpu().numpy()[0]

    # Internal â†’ External
    top_items_external = [dataset.id2token('item_id', int(iid)) for iid in top_items_internal]

    recommendations.append({
        'user_id': user_external,
        'item_ids': ' '.join(top_items_external)
    })

submission_df = pd.DataFrame(recommendations)

# í†µê³„
all_items = set()
for items_str in submission_df['item_ids']:
    all_items.update(items_str.split())

print(f"âœ… ì¶”ì²œ ìƒì„± ì™„ë£Œ")
print(f"   ì´ ì‚¬ìš©ì: {len(submission_df):,}ëª…")
print(f"   ê³ ìœ  ì•„ì´í…œ: {len(all_items):,}ê°œ")
print()

print(f"ğŸ“Š ì¶”ì²œ ë‹¤ì–‘ì„± ë¹„êµ:")
print(f"   ì´ì „ (v3): 10ê°œ ì•„ì´í…œ (0.31% ì»¤ë²„ë¦¬ì§€)")
print(f"   í˜„ì¬ (relaxed): {len(all_items):,}ê°œ ì•„ì´í…œ ({len(all_items)/df['item_id'].nunique()*100:.2f}% ì»¤ë²„ë¦¬ì§€)")
print()

if len(all_items) > 50:
    print(f"âœ… ì„±ê³µ! ì¶”ì²œ ë‹¤ì–‘ì„±ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
else:
    print(f"âš ï¸  ê²½ê³ : ì—¬ì „íˆ ì¶”ì²œ ë‹¤ì–‘ì„±ì´ ë‚®ìŠµë‹ˆë‹¤.")
print()

# ì €ì¥
t = pd.Timestamp.now()
output_dir = f"outputs/{t.year}-{t.month:02d}-{t.day:02d}"
os.makedirs(output_dir, exist_ok=True)
filename = f"{output_dir}/submit_MultiVAE_Relaxed_{t.year}{t.month:02d}{t.day:02d}{t.hour:02d}{t.minute:02d}{t.second:02d}.csv"

submission_df.to_csv(filename, index=False)

print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: {filename}")
print()

# ============================================================
# 8. Best Hyperparameters ì €ì¥
# ============================================================
import json

params_file = f"{output_dir}/best_hyperparams_multivae_relaxed_{t.year}{t.month:02d}{t.day:02d}{t.hour:02d}{t.minute:02d}{t.second:02d}.json"

params_data = {
    'hyperparameters': {
        'latent_dimension': int(best_config['latent_dimension']),
        'mlp_hidden_size': best_config['mlp_hidden_size'],
        'dropout_prob': float(best_config['dropout_prob']),
        'anneal_cap': float(best_config['anneal_cap']),
        'learning_rate': float(best_config['learning_rate']),
    },
    'validation_metrics': {
        'recall@5': float(best_metrics['recall@5']),
        'ndcg@5': float(best_metrics['ndcg@5']),
        'mrr@5': float(best_metrics['mrr@5']),
    },
    'diversity_metrics': {
        'unique_items': len(all_items),
        'coverage_pct': float(len(all_items) / df['item_id'].nunique() * 100),
    },
    'improvements': {
        'recall_improvement_pct': float(improvement),
        'diversity_improvement': f"{len(all_items)}ê°œ (ì´ì „: 10ê°œ)",
    }
}

with open(params_file, 'w') as f:
    json.dump(params_data, f, indent=2)

print(f"âœ… Hyperparameters ì €ì¥: {params_file}")
print()

# ============================================================
# ìµœì¢… ìš”ì•½
# ============================================================
print("=" * 80)
print("ìµœì¢… ìš”ì•½")
print("=" * 80)
print()

print(f"ğŸ¯ ì‹¤í—˜ ê²°ê³¼:")
print(f"   Validation Recall@5: {best_metrics['recall@5']:.4f} (ì´ì „: 0.0865)")
print(f"   ì¶”ì²œ ë‹¤ì–‘ì„±: {len(all_items):,}ê°œ (ì´ì „: 10ê°œ)")
print(f"   ê°œì„ ë„: {improvement:+.1f}%")
print()

print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼:")
print(f"   ì œì¶œ íŒŒì¼: {filename}")
print(f"   Hyperparameters: {params_file}")
print()

print(f"ğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
print(f"   1. ì œì¶œ íŒŒì¼ì„ Kaggleì— ì œì¶œí•˜ì—¬ Public LB í™•ì¸")
print(f"   2. Public LB > 0.20ì´ë©´ ì„±ê³µ!")
print(f"   3. ì‹¤í—˜ B (100% ë°ì´í„° ì¬í•™ìŠµ) ì§„í–‰")
print()

print("=" * 80)
print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("=" * 80)
